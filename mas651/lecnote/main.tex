\documentclass{memoir}
\usepackage{amsmath, amssymb, amsthm, enumitem}
\author{jaemin.oh}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}[definition]{Theorem}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\begin{document}
\title{2021s mas651 lecture note}
\maketitle
\tableofcontents

\chapter{Markov Chain}
\section{Construction, Markov Properties}
Let $(S, \mathcal{S})$ be a measurable space.
This will be a state space of our Markov Chain.

\begin{definition}[Transition Probability]
	A function $p:S \times \mathcal{S} \rightarrow \mathbb{R}$ is a transition probability if it satisfies two condition:
	\begin{enumerate}
		\item For each $B\in \mathcal{S}$, $p(\cdot, B)$ is a measurable mapping.
		\item For each $x \in S$, $p(x, \cdot)$ is a probability measure.
	\end{enumerate}
\end{definition}

\begin{remark}
	By the second condition, $p(\cdot, B)$ is a bounded measurable mapping.
	\label{transprob}
\end{remark}

If any transition probability is given, then we can define a Markov chain with a natural filtration $\mathcal{F}_n$.

\begin{definition}[Markov Chain]
	$(X_n, \mathcal{F}_n)$ is a markov chain with transition probability $p$ if
\[
	P\left( X_{n+1} \in B \lvert \mathcal{F}_n \right) = p\left( X_n, B \right).
\]
	
	\label{markovchain}
\end{definition}

After defining of Markov chain, we must have a curiosity about the existence.
The existence of $X_n$ can be shown by the Kolmogorov extension theorem.
First, assume an initial distribution $\mu$ on $S$.
Second, define
\[
	\nu_{0, \dots, n}\left( B_0, \dots B_n \right) = \int_{B_0}\mu(dx_0) \dots \int_{B_n}p(x_{n-1}, dx_n)
\]
Then $\left\{ \nu_{0, \dots n} \right\}_{n=0} ^\infty$ is a collection of finite dimensinal distributions which is consistent.
Thus, by the Kolmogorov extension theorem, there is a measure $P_\mu$ on $(\Omega_0, \mathcal{F}_{\infty}) = (S^\omega, \mathcal{S}^\omega)$ which satisfies
\[
	P_\mu \left (X_0 \in B_0 , \dots X_n \in B_n\right ) = \int_{B_0}\mu(dx_0)\dots \int_{B_n}p(x_{n-1}, dx_n)
\]
where $X_n$ is a coordinate map of $\omega \in \Omega_0$.

\begin{remark}
	$\omega = (\omega_0, \omega_1, \dots)$ so we can interpret $\omega_n$ as a $n$-th state of our markov chain.
	$\theta(\omega) = (\omega_n, \omega_{n+1}, \dots)$ is a shift operator, which makes the original $n$-th state be an initial.
	\label{<+label+>}
\end{remark}

We have constructed $P_\mu$ and corresponding $X_n$.
But we don't know whether $X_n$ is a markov chain or not.
To show that $X_n$ is actually a markov chain, we should use \ref{markovchain}.

\begin{theorem}
	$X_n$ above satisfies
\[
	P_\mu\left( X_{n+1} \in B \lvert \mathcal{F}_n \right) = p\left( X_n, B \right)
\]
	\label{<+label+>}
\end{theorem}
\begin{proof}
	To prove this, we must show
	\[
		\int_A 1_{\left( X_{n+1} \in B \right)}dP_\mu = \int_A p\left( X_n, B \right) dP_\mu
	\]
	for all $A \in \mathcal{F}_n$.
	Thanks to $\pi-\lambda$ theorem, we can restrict ourselves to $A = \left\{ X_0 \in B_0, \dots X_n \in B_n \right\}$.
	To simplicity, put $B = B_{n+1}$.
	Then the target equation is equal to
	\[
		P_\mu\left( X_0 \in B_0, \dots, X_{n+1} \in B_{n+1} \right) = \int_{B_0}\mu(dx_0)\dots\int_{B_n}p(x_{n-1}, dx_n)p(x_n, B_{n+1}).
	\]
	And we want that the last expression is equal to
	\[
		\int_A p(X_n, B)dP_\mu.
	\]
	To do this, first replace $p(x_n, B_{n+1})$ to $1_C(x_n)$.
	We can easily check that the equality is true for any indicator function.
	Thus the equality is true for any simple function, and true for any bounded measurable function by BCT.
	Note that $p(\cdot, B_{n+1})$ is a bounded measurable mapping.

\end{proof}

To go further, we need a very useful theorem which is called the Monotone Class Theorem.

\begin{theorem}[Monotone Class Theorem]
	Let $\mathcal{A}$ be a $\pi$-system that contains $\Omega$ and let $\mathcal{H}$ be a collection of real valued functions which satisfies:
	\begin{enumerate}
		\item If $A \in \mathcal{A}$, then the corresponding indicator function belongs to $\mathcal{H}$.
		\item If $f, g\in \mathcal{H}$, then for any real linear combination of them belongs to $\mathcal{H}$.
		\item If $0\leq f_n \uparrow f \leq M$ and $f_n \in \mathcal{H}$, then $f \in \mathcal{H}$.
	\end{enumerate}
	Then $\mathcal{H}$ contains all $\sigma(\mathcal{A})$ measurable functions which are bounded.
	\label{monotoneclassthm}
\end{theorem}

\begin{proof}
	$\mathcal{G} = \left\{ A:1_A \in \mathcal{H} \right\}$ is a $\lambda$-system containing $\mathcal{A}$.
	Thus $\sigma(\mathcal{A}) \subset \mathcal{G}$ which means $1_A \in \mathcal{H}$ for all $A \in \sigma(\mathcal{A})$.
	Then $\mathcal{H}$ contains all simple functions which are $\sigma(\mathcal{A})$ measurable, hence the conclusion follows from the third condition above.
\end{proof}

\begin{remark}
	A class of bounded measurable function $f:S \rightarrow \mathbb{R}$ which satisfies
	\[
		E\left( f(X_{n+1}) \lvert \mathcal{F}_n \right) = \int p(X_n, dy)f(y)
	\]
	is actually a monotone class. 
	Because \ref{markovchain} says the first condition for $\mathcal{A} = \mathcal{S}$,
	and the others are satisfied by the elementary properties of expectation and integration.

	By induction, we can extend this result by
	\[
		E\left[ \prod_{m=0}^n f_m(X_m) \right] = \int\mu(dx_0)f_0(x_0) \dots \int p_{n-1}(x_{n-1}dx_n)f_n(x_n)
	\]
	where the subscript for $p$ stands for temporally inhomogeneous transition probability.
	\label{<+label+>}
\end{remark}

\begin{theorem}[The Markov Property]
	Let $Y: \Omega_0 \rightarrow \mathbb{R}$ be any bounded measurable mapping.
	Then $E_\mu \left (Y \circ \theta_m \lvert \mathcal{F}_m \right ) = E_{X_m}\left[ Y \right].$
	\label{markovproperty}
\end{theorem}

\begin{proof}
	First, assume $Y = \prod_{k=0}^n g_k$ where $g_k$ are bounded measurable.
	By using Dynkin's theorem and the previous result, we can prove the Markov property for this case.
	We can extend the special case to general case by using the monotone class theorem.
\end{proof}

\begin{remark}
	$\mathcal{F}_m$ is an information accumulated during time $0$ to $m$.
	Conditioning on this information, any future($\theta_m$, starting from time $m$) quantity can be guessed by the formula.
	It seems somewhat related to the elementary Markov property.
	Actually this result implies the usual Markov property.
	Let's see.

	Let $Y = 1_{(X_n = j)}$.
	Then
	\[
		\begin{split}
			&P\left( X_n = j \lvert X_0 = i_0, \dots, X_{n-1} = i_{n-1} \right)\\
			&= \frac{P\left( X_0 = i_0, \dots, X_n = j \right)}{P\left( X_0 = i_0, \dots, X_{n-1} = i_{n-1} \right)}\\
			&= \frac{\int_{X_0 = i_0, \dots, X_{n-1} = i_{n-1}}P\left( X_n = j \lvert \mathcal{F}_{n-1} \right)dP}{P\left( X_0 = i_0, \dots, X_{n-1} = i_{n-1} \right)} \\
			&= \frac{\int_{X_0 = i_0, \dots, X_{n-1} = i_{n-1}}E\left (Y\circ \theta_{n-1} \lvert \mathcal{F}_{n-1}\right )}{P(X_0 = i_0, \dots, X_{n-1} = i_{n-1})} \\
			&=  P\left( X_n =j \lvert X_{n-1} = i_{n-1} \right).
		\end{split}
	\]
	\label{<+label+>}
\end{remark}

\begin{theorem}[Chapman-Kolmogorov]
	Let $X_n$ be a Markov chain.
	Then
	$P_x\left( X_{m+n} = z \right) = \sum_{y}P_x\left( X_m = y \right)P_y\left( X_n = z \right).$
	\label{ChapmanKolmogorov}
\end{theorem}

\begin{proof}
	This is a direct consequence of the Markov property.
	Use this:
	\[
		P_x(X_{m+n} = z) = E_x P_x\left( X_{m+n} = z \lvert \mathcal{F}_m \right).
	\]
\end{proof}

\begin{remark}
	This is intuitively clear.
	To go $z$ from $x$ in exactly $m+n$ steps, we must pass $y$ in $m$-th step.
	When a state of $m$-th step is given, remaining $n$ steps are independent of the past by the Markov property.
	\label{<+label+>}
\end{remark}

Now recall the optional stopping theorem in the martingale theory.
That theorem says us that in certain condition, we can regard a stopping time as an index of our martingale.
Analogy in Markov chain will be shown.

Let $N$ be any stopping time.
Note that $N$ is a stopping time if and only if $\left\{ N= n \right\} \in \mathcal{F}_n$.

\begin{definition}
	$\mathcal{F}_N$ is a collection of set $A$ such that $A \cap \left\{ N=m \right\} \in \mathcal{F}_m$ for all $m$.
	Also, $\theta_N(w)$ is $\theta_n(w)$ when $N=n$ and $\Delta$ when $N=\infty$.
	Here, $\Delta$ is an extra point added to $\Omega_0$.
	Since we will only cover the case $N<\infty$, $\Delta$ will make no problem.
	\label{<+label+>}
\end{definition}

\begin{theorem}[The Strong Markov Property]
	Let $Y_n : \Omega_0 \rightarrow \mathbb{R}$ be uniformly bounded measurable function.
	Then
	\[
		E_\mu \left( Y_N \circ \theta_N \lvert \mathcal{F}_N \right) = E_{X_N}\left[ Y_N \right]
	\]
	on $\left\{ N < \infty \right\}$.
	\label{strongmarkovproperty}
\end{theorem}

\begin{proof}
	We can write $1_{\left( N < \infty \right)} = \sum_{n=0}^\infty 1_{\left( N= n \right)}$.
	Let $A \in \mathcal{F}_N$.
	Then it can be easily shown that
	\[
		E_\mu \left[ Y_N \circ \theta_N ; A \cap \left\{ N < \infty \right\} \right]
		= E_\mu \left[ E_{X_N}\left[ Y_N \right]; A \cap \left\{ N< \infty \right\} \right].
	\]
\end{proof}

\begin{remark}
	On $\left\{ N < \infty \right\}$ can be thought of as multiplying both sides by $1_{\left( N<\infty \right)}$.
	\label{<+label+>}
\end{remark}

	An application of the strong Markov property is:
\begin{theorem}
	$P_x\left( T_y^k < \infty \right) = \rho_{xy} \rho_{yy}^{k-1}$ for $k \ge 1$.
	\label{<+label+>}
\end{theorem}

\begin{proof}
	Let $Y = 1$ if $w_n = y$ for some $n$ and $0$ otherwise.
	Take $N = T_y^{k-1}$.
	Then $Y \circ N = 1_{\left( T_y^k < \infty \right)}.$
	Use this, the strong Markov property, and the induction.
\end{proof}

\begin{theorem}[Reflection Principle]
	Let $\left\{ \xi_i \right\}_{i=1}^\infty$ be an iid sequence of random variables, with symmetric distribution about $0$.
	Let $S_n = \sum_{i=1}^n \xi_i$.
	Then $S_n$ is clearly Markov chain.
	If $a>0$, then
	\[
		P\left( \sup_{m \le n} S_m \ge a \right) \le 2 P\left( S_n \ge a \right).
	\]
	\label{reflectionprinciple}
\end{theorem}

\begin{proof}
	temporally omitted.
\end{proof}<++>

\begin{remark}
	This is a kind of maximal inequality.
	The maximal quantity of the random variable during time $n$ can be estimated by the quantity of the random variable at time $n$.
	\label{<+label+>}
\end{remark}


\end{document}
